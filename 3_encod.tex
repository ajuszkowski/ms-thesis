\chapter{Portability analysis as an SMT~problem}
\label{ch:port}


% TODO: Add intro to the bounded reachability analysis using SAT -- ? 

% TODO: other tools for bounded model-checking: 


%TODO: the method used by Porthos is complete and efficient, but the implementation sucks. Why? 1), 2), 3). How can it be fixed? 1), 2), 3).

%TODO: more about SMT-(= SAT + constraints expressed in background theories. Exapmles of theories that we use and that are implem-ted in Z3).

As it has been discussed in Chapter~\ref{ch:intro}, the program may behave differently when compiled for different parallel hardware architectures.
This may cause the portability bugs, the behaviour that is allowed under one architecture and forbidden under another. 
%already said:
%Although the research of weak memory models has achieved considerable success, most works remain to be rather theoretical that practical and serve mostly as tools for better understanding the concurrent nature of programs.
% The first tool that tackles the problem 
%The first work that has investigated the practical approach of modelling and verification the real-world programs with respect to weak memory models was ~\cite{Porthos17a}  
In this Chapter, we describe the general task of analysing the concurrent software portability
%may be stated 
as a \textit{bounded reachability} problem, which in turn can be reduced to a SMT problem~\cite{Porthos17a}.

%TODO: sound, complete...

\section{Model checking and reachability analysis}
\label{ch:port:mc}

The model checking is the problem of verifying the system (the model) against a set of constraints (the specification)~\cite{dkw2008}.
As the state machine model is the most widespread mathematical model of computation, most classical model checking algorithms explore the state space of a system in order to find states that violate the specification.

The general scheme of model checking is the following. 
The analysing system (the \textit{model}) is represented as a transition system, a directed graph with labelled nodes representing states of the system.
Each state corresponds to the unique subset of atomic propositions that characterise its behavioural properties.
Once the model has been constructed, it can be checked for compliance to the \textit{specification}.

Usually, the specification defines temporal constraints over the properties of the system.
For instance, the specification assert may state that the property \textit{always} holds (the \textit{safety}) or the property will \textit{eventually} hold (the \textit{liveness}).
Commonly, the \textit{Linear Temporal Logic~(LTL)} or \textit{Computational Tree Logic~(CTL)} (along with their extensions) is used as a specification language due to the expressiveness and verifiability of their statements.

In the described scheme, the model checking problem is reducible to the reachability analysis, an iterative process of a systematic exhaustive search in the state space.
This approach is called \textit{Unbounded Model Checking~(UMC)}.
However, all model checking techniques are exposed to the \textit{state explosion problem} as the size of the state space grows exponentially with respect to the number of state variables used by the system (its size).
In case of modelling concurrent systems, this problem becomes much more considerable due to exponential number of possible interleavings of states.
Therefore, the research in model checking over past 40 years was fighting the state explosion problem mostly by optimising search space, search strategy or basic data structures of existing algorithms.

One of the first techniques that optimises the search space considerably is the symbolic model checking with \textit{Binary Decision Diagrams~(BDDs)}.
Instead of processing each state individually, in this approach the set of states is represented by the BDD, a data structure that allow to perform operations on large boolean formulas efficiently~\cite{clarke2012model}.
The BDD representation can be linear of size of variables it encodes if the ordering of variables is optimal, otherwise the size of BDD is exponential.
The problem of finding such an optimal ordering is known as NP-complete problem, which makes this approach inapplicable in some cases.

The other idea is to use satisfiability solvers for symbolic exploration of state space~\cite{clarke2001bounded}.
In this approach, the state space exploration consists of the sequence of queries to the SAT-solver, represented as boolean formulas that encode the constraints of the model and the finite path to a state in the corresponding transition system.  
%This approach uses an iterative process of constructing queries to the SAT-solver as a boolean formula which encodes the constraints of the model and the finite path to a state in the corresponding transition system. 
%Due to the SAT-solver.
This technique is called \textit{bounded model checking (BMC)} as the search process is being repeated up to the user-defined bound $k$, which may result to incomplete analysis in general case.
However, there exist numerous techniques for making BMC complete for finite-state systems~(e.g.,~\cite{shtrichman2000tuning}).
%As well as the BMC problem, the approach used by the \porthos{}

%For instance, the idea of grouping states with similar properties into equivalence classes lead to the concept of traces in concurrent systems proposed by A.~Mazurkiewicz in 1986~\cite{mazurkiewicz1986trace}. 


\section{Portability analysis as a bounded reachability problem}
\label{ch:port:enc}

%TODO: overapproximation. loops, Rice's thm. Within the bound, the method is complete.

In general, a BMC problem aims to examine the non-reachability of the "undesirable" states of a finite-state system.
Let $\vec{x} = (x_1, x_2, ..., x_n)$ be a vector of $n$ variables that uniquely distinguishes states of the system; let $Init(\vec{x})$ be an \textit{initial-state predicate} that defines the set of initial states of the system; let $Trans(\vec{x}, \pvec{x}')$ be a \textit{transition predicate} that signifies whether there the transition from state $\vec{x}$ to state $\pvec{x}'$ is valid; let $Bad(\vec{x})$ be a \textit{bad-state predicate} that defines the set of undesirable states.
Then, the BMC problem, stated as the reachability of the undesirable state withing $k$ steps, is formulated as following:
%\begin{equation}
$\mathtt{SAT}( Init(\vec{x_0}) \land Trans(\vec{x}_0, \vec{x}_1) \land ... \land Trans(\vec{x}_{k-1}, \vec{x}_k) \land Bad(\vec{x}_k) )$.
%\end{equation}

The portability analysis problem may also be stated as a reachability problem, where the undesirable state is one reachable under the target~$\mathcal{M_T}$ memory model and unreachable under the source memory model~$\mathcal{M_S}$.
%However, unlikely a BMC problem, the portability analysis does not require to call the SMT-solver repeatedly, since (imperative) programs may be converted as acyclic state graph (by reducing the loops, see Section~\ref{ch:impl:proc:x-transf}) and the $Trans$ predicate may be stated only for the final state of a program.
Consider the function $cons_{\mathcal{M}}(P)$ calculates the set of executions of program $P$ consistent under the memory model $\mathcal{M}$. Then, the program $P$ is called portable from the source architecture (memory model) $\mathcal{M_S}$ to the target architecture $\mathcal{M_T}$ if all executions consistent under $\mathcal{M_T}$ are consistent under $\mathcal{M_S}$~\cite{Porthos17a}:

\begin{definition}[Portability]
Let $\mathcal{M_S}$, $\mathcal{M_T}$ be two weak memory models. A program $P$ is portable from $\mathcal{M_S}$ to $\mathcal{M_T}$ if 
$cons_{\mathcal{M_T}}(P) \subseteq cons_{\mathcal{M_S}}(P)$
\end{definition}

Note that the definition of portability requirements against \textit{executions} is strong enough, as it implies the portability against \textit{states} (the \textit{state-portability})~\cite{Porthos17b}.
The result SMT-formula $\phi$ that encodes the portability problem should contain both encodings of control-flow $\phi_{CF}$ and data-flow $\phi_{DF}$ of the program, and assertions of both memory models: ${\phi \defeq \phi_{CF} \land \phi_{DF} \land \phi_{\mathcal{M_T}} \land \phi_{\lnot\mathcal{M_S}}}$. If the formula is satisfiable, there exist a portability bug.
%The control-flow and data-flow encodings are standard for BMC~\cite{collavizza2006exploration}, they are described below. 
%However, encoding of memory models requires additional techniques due to recursive definitions of relations, that were proposed in~\cite{Porthos17a}.


\subsection{Encoding for the control-flow} %Encoding the control-flow constraints}
\label{ch:port:enc:cf}

The control-flow of a program is represented by the \textit{control-flow graph}, a directed acyclic connected graph with single source and multiple sink nodes. %TODO: footnote why multiple sinks
In a control-flow graph, there are two types of edges (transitions): \textit{primary transitions} that denote unconditional jumps or if-true-transitions (pictured with solid lines), and \textit{alternative transitions} that denote if-false-transitions (pictured with dotted lines).
Each node on graph can have either one successor (primary) or two successors (both primary and alternative); only computation events can serve as a branching point).
However, each merge node can have any positive number of predecessors, where each edge may be either primary or alternative.

While working on the \porthos[2], we applied some modifications of the encoding scheme for the control-flow.
The changes are conditioned by the need to be able to process an arbitrary control-flow produced by conditional and unconditional jumps of C language.
For that, we compile the recursive abstract syntax tree~(AST) of the parsed C-code to the plain (non-recursive) event-flow graph.
We show %TODO! to show, actually!!!
that the new encoding is smaller than the old one used in \porthos{} since it does not produces new variables for each high-level statement of the input language.
%The \tool{PORTHOS} tool encodes the \textit{instructions}, which have recursive nature, in a recursive manner. This means, it adds synthetic composite instructions for both linear (sequential) and non-linear (branching) instructions. 
For instance, \porthos{} uses the encoding scheme where the control-flow of the sequential instruction $i_1 = i_2; i_3$ was encoded as
$\phi_{CF}(i_2;i_3) \defeq (cf_{i_1} = (cf_{i_2} \land cf_{i_3})) \land \phi_{CF}(i_2) \land \phi_{CF}(i_3)$,
and control-flow of the branching instruction $i_1 = (c \; ? \; i_2 \; : \; i_3)$ was encoded as
$\phi_{CF}( \; c \; ? \; i_2 \; : \; i_3) \defeq (cf_{i_1} \Leftrightarrow (cf_{i_2} \lor cf_{i_3})) \land \phi_{CF}(i_2) \land \phi_{CF}(i_3)$
(here we used the notation of C-like ternary operator `\texttt{x?y:z}' for defining the conditional expression `$\mathtt{if}\;x\;\mathtt{then}\;y\;\mathtt{else}\;z$').
In contrast, the new scheme implemented in \porthos[2] firstly compiles the recursive high-level code into the linear low-level event-based representation, that is then encoded into an SMT-formula. The encoding of branching nodes depends on the \textit{guards}, the value of conditional variable on the branching state, which in turn is encoded as data-flow constraint (see Section~\ref{ch:port:enc:df}).

Let $\fx:\,\mathbb{E}\,\rightarrow\,\{0,1\}$ be the predicate that signifies the fact that the event has been e\textbf{x}ecuted (and, consequently, has changed the state of the system).
% TODO: attention to these non-breakable spaces
Let $\fv:\,\mathbb{C}\,\rightarrow\,\mathbb{R}$ be the function that returns the value of the computation event (evaluates it) that will be computed once the event is executed (strictly speaking, it returns the \textit{set} of values determined by the \rf-relation; see Section~\ref{ch:wmm:model:relations} for details on relations).
We distinguish the function ${\fv_g\;:\;\mathbb{C}_g\;\rightarrow\;\{0,1\}}$ that evaluates the guard computation event. In the result formula, all symbols $\fx(e_i)$ and $\fv(e_i)$ are encoded as boolean variables.

\begin{figure}[b]
  \centering
  \begin{minipage}{\textwidth}
      \begin{subfigure}[b]{0.3\textwidth}
        \makebox[\textwidth]{
        \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1.5cm,semithick]
            \node[c] (1) [] {$e_1$};
            \node[c] (2) [below of=1] {$e_2$};
            \path[->]
            (1) edge [] node {} (2)
            ;
        \end{tikzpicture}
        }
        \caption{Sequential events (\texttt{seq})}
        \label{encode:cf:seq}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.35\textwidth}
        \makebox[\textwidth]{
        \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1.5cm,semithick]
            \node[c] (1) [] {$e_1$};
            \node[c] (2) [below left of=1] {$e_2$};
            \node[c] (3) [below right of=1] {$e_3$};
            \path[->]
            (1) edge [] node {} (2)
            (1) edge [dotted] node {} (3)
            ;
        \end{tikzpicture}
        }
        \caption{Conditional branching (\texttt{br})}
        \label{encode:cf:br}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.25\textwidth}
        \makebox[\textwidth]{
        \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1.5cm,semithick]
            \node[c] (i) [] {$e_i$};
            \node[c,draw=none] (ii) [right=0.2cm of i] {};
            \node[c,draw=none] (iii) [right=0.2cm of ii] {$...$};
            \node[c,draw=none] (iv) [right=0.2cm of iii] {};
            \node[c] (v) [right=0.2cm of iv] {$e_{i+j}$};
            \node[c] (k) [below of=iii] {$e_k$};
            \path[->]
            (i) edge [] node {} (k)
            (ii) edge [] node {} (k)
            (iii) edge [dotted] node {} (k)
            (iv) edge [dotted] node {} (k)
            (v) edge [] node {} (k)
            ;
        \end{tikzpicture}
        }
        \caption{Branch merging (\texttt{mer})}
        \label{encode:cf:merge}
    \end{subfigure}
    %
    \caption{Linear and non-linear cases of a control-flow graph}
    \label{encode:cf}
\end{minipage}
\end{figure}

Consider the possible mutual arrangements of nodes in a control-flow graph presented in Figure~\ref{encode:cf}.
For these cases, we propose the encoding scheme that uniquely encodes each node of graph and at the same time allows to encode partially executed program.
Equation~\ref{enc:cf:seq} encodes the sequential control-flow represented in Figure~\ref{encode:cf:seq} and reflects the fact that the event $e_2$ can be executed iff the event $e_1$ has been executed.
Equation~\ref{enc:cf:br} encodes the branching control-flow depicted in Figure~\ref{encode:cf:br} by allowing only following executions: $\{\emptyset, \; (e_1), \; (e_1\,\rightarrow\,e_2), \; (e_1\,\rightarrow\,e_3)\}$.
In Equation~\ref{enc:cf:merge}, which encodes the control-flow of a merge-point represented in Figure~\ref{encode:cf:merge}, the event $e_k$ is executed if either of its predecessors has been executed, regardless the type of the transition.
%TODO: maybe add the overall formula \phi_{CF} ?

\begin{align}
\phi_{CF_{seq}}   \defeq \ & \ \fx(e_2) \rightarrow \fx(e_1) \label{enc:cf:seq} \\
\phi_{CF_{br}}    \defeq \ & \ [\fx(e_2) \rightarrow \fx(e_1)] \land [\fx(e_3) \rightarrow \fx(e_1)] \land \nonumber \\
                      & \ [\fx(e_2) \rightarrow \fv(e_1)] \land [\fx(e_3) \rightarrow \lnot\fv(e_1)] \land \nonumber \\
                      & \lnot [\fx(e_2) \land \fx(e_3)]  \label{enc:cf:br} \\
\phi_{CF_{mer}} \defeq \ & \fx(e_k) \rightarrow (\bigvee\limits_{e_p \in \mathtt{pred}(e_k)}^{} \fx(e_p)) \label{enc:cf:merge}
\end{align}


For sake of correctness of the encoding, we require all branches to have at least one event.
Thus, for branching statements that do not have any events in one of the branches (such a branch represents a conditional jump forward), we add the synthetic \texttt{nop}-event as it is shown in Figure~\ref{encode:branching:nop}.

\begin{figure}
    \centering
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1.5cm,semithick]
        \node[c] at (0,0) (1a) {$e_1$};
        \node[c] at (0,-1.5) (2a) {$e_2$};
        \node[c] at (0,-3) (3a) {$e_3$};
        \node[c,draw=none] at (3,-1.5) (arr) {$\longrightarrow$};
        \node[c] at (6,0) (1b) {$e_1$};
        \node[c] at (6,-1.5) (2b) {$e_2$};
        \node[c,fill=black!15] at (7.5,-1.5) (nop) {$e_{\mbox{\tiny NOP}}$};
        \node[c] at (6,-3) (3b) {$e_3$};
        \path[->]
        (1a) edge [] node {} (2a)
        (2a) edge [] node {} (3a)
        (1a) edge [dotted,bend left=60] node {} (3a)
        (1b) edge [] node {} (2b)
        (2b) edge [] node {} (3b)
        (1b) edge [dotted,bend left] node {} (nop)
        (nop) edge [dotted,bend left] node {} (3b)
        ;
    \end{tikzpicture}
    \caption{Transformation of the empty-branch nonlinear control-flow}
    \label{encode:branching:nop}
\end{figure}


\subsection{Encoding for the data-flow}
\label{ch:port:enc:df}

To encode the data-flow constraints, we use the \textit{Static Single-Assignment~(SSA) form} in order to be able to capture an arbitrary data-flow into a single SMT-formula.
The SSA form requires each variable to be assigned only once within entire program.
In contrast, \porthos{} used the \textit{Dynamic Single-Assignment~(DSA)} form, that requires indices to be unique within a branch.
Although the number of variable references (each of which is encoded as unique SMT-variable) on average is logarithmically less in case of the DSA form than the SSA form, the result SMT-formula still needs to be complemented by same number of equality assertions when encoding the data-flow of merge points~\cite{Porthos17a}.

Following~\cite{Porthos17b}, the indexed references of variables are computed in accordance with the following rules:
%\begin{enumerate}[noitemsep,topsep=0pt]
(1)~any access to a shared variable (both read and write) increments its SSA-index;
(2)~only writes to a local variable increment its SSA-index (reads preserve indices);
(3)~no access to a constant variable or computed (evaluated) expression changes their %SSA-index.
%\end{enumerate}
These rules determine the following encoding of load, store and computation events within single thread:
%TODO: maybe add the overall formula \phi_{DF} ?
%
\begin{align}
    \phi_{DF_{e = \mathtt{load}(r \leftarrow l)}} \defeq \ & \fx(e) \rightarrow (r_{i+1} = l_{i+1}) \\
    \phi_{DF_{e = \mathtt{store}(l \leftarrow r)}} \defeq \ & \fx(e) \rightarrow (l_{i+1} = r_i) \\
    \phi_{DF_{e = \mathtt{eval}(\cdot)}} \defeq \ & \fx(e) \rightarrow \fv(e) % \\
\end{align}

To convert the program into SSA form, for each event each variable that is declared so far (either local or shared) is mapped to its indexed reference; this information is stored in the SSA-map "event \textit{to} variable \textit{to} SSA-index". %for each event stores the map that each declared variable maps to its SSA-index
%"$\{$ event : $\{$ variable : index $\}\}$",
%"$\left\{$ event : $\left\{$ variable : index $\right\} \right\}$",
The SSA-map is computed iteratively while traversing the event-flow graph in topological order as it is described in Algorithm~\ref{algorithm:ssa-map}.

\begin{algorithm}
    \caption{Algorithm for computing the SSA-indices}\label{alg:compute-ssa}
    \algorithmicrequire{The event-flow graph $G = \langle N, E \rangle$ where $V$ is the set of nodes (events), $E$ is the set of control-flow transitions, $e_0$ is the entry node}
    \algorithmicensure{The SSA-map of the form "$\{$ event : $\{$ variable : index $\}\}$"}
    \begin{algorithmic}[1]
        \Function{Compute-SSA-Map}{G}
            \State {$S \gets$ empty map; $S[e_0] \gets$ empty map}
            \ForAll {event $e_i \in G.N$ in topological order}
                \ForAll {predecessor $e_j \in \mathtt{pred}(e_i)$}
                    \State {$S[e_i] \gets \mathtt{copy}(S[e_j])$}
                    \ForAll {variable $v_k \in$ set of variables accessed by $e_i$ }
                        \State {$S[e_i][v_k] \gets \mathtt{max}(S[e_i][v_k], \ S[e_j][v_k])$}
                        \If {need to update the index of $v_k$} \Comment {cases (1)-(2)}
                            \State {$S[e_i][v_k] \gets S[e_i][v_k] + 1$}
                        \EndIf
                    \EndFor
                \EndFor
            \EndFor
        \EndFunction
    \end{algorithmic}
    \label{algorithm:ssa-map}
\end{algorithm}

The time of described algorithm is linear of the size of event-flow graph since it performs only single traverse of the graph.
%Notwithstanding the overhead of storing (or, equivalently, computing with %TODO: are you sure that 'with'?
%linear time) the predecessors of each node in order to be able to convert the program into an SSA form, the time of such a transformation reduced %TODO: How much ????
%comparing to the algorithm implemented in \porthos, which recomputed SSA-indices recursively for each instruction. %TODO: PROOF OR REMOVE !  Maybe, some words of copying, merging SSA maps ?

As it has been described before, the \rf-relation links data-flow between events %TODO: another word?
of data-flow stored in equivalence assertions over the SSA-variables. 
%The encoding of the \rf-relation, which links the SSA-variables to the original variables, 
The encoding of this linkage left untouched as it is implemented in \porthos: for each pair of events $e_1$ and $e_2$ linked by the \rf-relation, we add the following constraint:
%
\begin{align}
\phi_{DF_{mem}}(e_1, e_2) \defeq \ & \mathtt{rf}(e_1, e_2) \rightarrow (l_i = l_j) 
\end{align}

where the variable of location $l$ is mapped to the SSA-variable $l_i$ for event $e_1$, and to the SSA-variable $l_j$ for event $e_2$; and the predicate $\mathtt{rf}(e_1, e_2)$ is encoded as a boolean variable, which itself equals $true$ if $e_2$ reads the shared variable that was written in $e_1$.


\subsection{Encoding for the memory model} %Encoding the memory model constraints}
\label{ch:port:enc:wmm}

The basic scheme for encoding the memory model is proposed in~\cite{Porthos17a}.
The encoding consists of two parts: encoding the \textit{derived relations} and encoding the memory model \textit{assertions}.

In an SMT-formula, the relation $x \relation{r} y$ is represented by a boolean variable $r(x, y)$ that indicates whether the relation holds.
The derived relations are encoded by fresh boolean variables according to the following rules~\cite{Porthos17b}:
\begin{itemize}[noitemsep,topsep=0pt]
\item $r_1 \cup r_2(e_1, e_2) \defeq r_1(e_1, e_2) \lor r_2(e_1, e_2)$
\item $r_1 \cap r_2(e_1, e_2) \defeq r_1(e_1, e_2) \land r_2(e_1, e_2)$
\item $r_1 \relminus r_2(e_1, e_2) \defeq r_1(e_1, e_2) \land \neg r_2(e_1, e_2)$
\item $r^{-1}(e_1, e_2) \defeq r(e_2, e_1)$
\item $r^{*}(e_1, e_2) \defeq r^{+}(e_1, e_2) \lor (e_1 = e_2)$
\item $r_1;r_2(e_1,e_2) \defeq \bigvee\limits_{e_k \in \mathbb(E)}^{} r_1(e_1,e_k) \land r_2(e_k,e_2)$
\item $r^{+}(e_1, e_2) \defeq \mathtt{tc}_{\left\lceil log|\mathbb{E}| \right\rceil} (e_1, e_2), where \\
\hspace*{1em} \mathtt{tc}_{0}(e_1,e_2) \defeq r(e_1,e_2), and \\ 
\hspace*{1em} \mathtt{tc}_{i+1}(e_1,e_2) \defeq r(e_1,e_2) \lor (\mathtt{tc}_{i}(e_1,e_2);\mathtt{tc}_{i}(e_1,e_2))$
\end{itemize}

Note that \cat{} language allows mutually-recursive definitions of relations (for example, `$\mathtt{r_1} = \mathtt{r_2} \cup (r_1;r_1)$').
The basic idea of using the Kleene fixpoint iteration for encoding such relations was also proposed in~\cite{Porthos17a}: for any pair of events $e_1,e_2 \in \mathbb{E}$ and relation $r \subseteq \mathbb{E} \times \mathbb{E}$, we encode a new integer variable $\Phi_{e_1,e_2}^{r}$ that represents the round of Kleene iteration on which the variable $r(e_1, e_2)$ has been set.
 %is used for guessing the execution 

The memory model can assert acyclicity, irreflexivity of emptiness of a relation or a set of events.
As it has been proposed in~\cite{Porthos17a}, encoding the acyclicity assertion uses numerical variable $\Psi_e \in \mathbb{N}$ for each event $e$ in the relation to be asserted: $acyclic(r) \defeq (r(e_1,e_2) \Rightarrow (\Psi_{e_1} < \Psi_{e_2}))$.
The irreflexivity assertion as $irreflexive(r) \defeq \bigvee\limits_{e_k \in \mathbb(E)}^{} \lnot r(e_k,e_k)$.

%The ... \wmodel{}

%Most bounded model-checking problems are solved via computing the fixpoint for the

%The encoding of a weak memory model itself does not depend on the analysing program. The problem here...
%recursive definitions and Kleene iteration

%r 1 ∪r 2 (e 1 , e 2 ) ⇔ r 1 (e 1 , e 2 ) ∨ r 2 (e 1 , e 2 )
%r 1 ∩r 2 (e 1 , e 2 ) ⇔ r 1 (e 1 , e 2 ) ∧ r 2 (e 1 , e 2 )
%r 1 \r 2 (e 1 , e 2 ) ⇔ r W
%r −1 (e 1 , e 2 ) ⇔ r(e 2 , e 1 )
%1 (e 1 , e 2 ) ∧ ¬r 2 (e 1 , e 2 )
%r 1 (e 1 , e 3 ) ∧ r 2 (e 3 , e 2 )
%r ∗ (e 1 , e 2 ) ⇔ r + (e 1 , e 2 ) ∨ (e 1 = e 2 )
%r 1 ;r 2 (e 1 , e 2 ) ⇔
%e 3 ∈E
%r + (e 1 , e 2 ) ⇔ tc ⌈log |E|⌉ (e 1 , e 2 ), where
%tc 0 (e 1 , e 2 ) ⇔ r(e 1 , e 2 ), and
%tc i+1 (e 1 , e 2 ) ⇔ r(e 1 , e 2 ) ∨ (tc i (e 1 , e 2 ); tc i (e 1 , e 2 ))
%
%As it is described in~\cite{Porthos17a}, the is based . . .
%~\cite{Porthos17a}%, Section "Programs and Memory Models", subsection "Encoding Derived Relations"
