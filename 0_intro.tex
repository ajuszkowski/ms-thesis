\chapter{Introduction}
\label{section:introduction}

Most modern computer systems contain large parts working concurrently. Though system parallelising can improve its performance drastically, it opens numerous of problems connected to correctness, robustness and reliability, which makes the concurrent program design one of the most difficult areas of programming~\cite{mckenney2017parallel}.
% Most articles, presentations and books on concurrent programming start with words how hard it is.

Traditionally, studies related to concurrent programming concern mostly classical cases of implementing race-free and lock-free parallel programs, asynchronous data structures and synchronisation primitives of a programming language. Unfortunately, 
when it comes to 
%in cases of 
analysis of the real-world concurrent programs, the algorithmic level of abstraction is not enough for guaranteeing their properties of correctness and reliability. The reasons of this fact lie in the code optimisations that both compiler and hardware perform in order to increase performance as much as possible. For instance, some processor architecture performs reordering of memory access instructions in a thread, while it should not be considered as a valid code transformation with respect to the logic of program (//TODO: example). It is said that this kind of processor exploits \textit{relaxed}, or \textit{weak memory model~(WMM)}, that allow certain amount of memory operations reordering while preserving consistency. Weak memory models serve as set of guarantees made by designers of hardware (programming language, compiler, operation environment, etc.) for programmers on which behaviours of their concurrent code they should expect.

In last decades, the problem of formalising WMMs of different 
%hardware
architectures has been developed significantly. The research is aimed, firstly, at formalising the weak memory models, providing systematic, sound and complete formal approach of defining WMMs in order to be able to verify systems with respect to these models. Secondly, the researchers work on extracting hardware memory models from the public specifications written in natural language and thus suffering from ambiguities and incompletenesses. Thirdly, important research direction targets the problem of verifying (or at least finding bugs in) existing software systems with respect to weak memory models. 

while the first two have been developed .... 
The latter approach still suffers from the exponent.... (need this?)
Kernel.


wmm as a formal way to define guaranties that a hw, programming language, execution environment provide for programmers.

considering wmm as a set of allowed behaviours, the latter wmms are the supersets

wmm allows and disallows optimisations: partial sync of memory buffers, out-of-order execution (reordering), <more> => more behaviours that are unallowed in SC.

question possible to answer with wmm: which behaviours (in addition to SC) are allowed? which new states are allowed? Consequently, correctness, absence of data races, deadlocks or portability issues, etc.

existing tools: herd, diy7 -- exhaustive search approach for exploring the state space.

another approach: using smt solver, e.g. for answering questions like 

base paper: aims to investigate portability of small programs written in a C-like pseudocode and provides the proof-of-concept tool PORTHOS [link]. As input, it takes a program and two memory models in CAT language. Then it encodes programs and memory models into an smt formula and tries to solve it via z3. Current thesis aims to extend this tool functionality to process the real C code, therefore it proposes different modula program architecture and multiple optimisations.



\section{Thesis structure}
\label{section:introduction:structure}

...